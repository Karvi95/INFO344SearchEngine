---Azure Setup---
Section 0: Had to set up Generalized Storage and obtain the connection string to put into the config file.

---Coding---
-Section 1: Table Classes
Based on the inclass assignment where we had to save the sum of numbers into an Azure table, I realized that I would need as many classes as there were items to keep track of. Therefore, I created a WebPage Class, an Error Class, a Status Class, and a Performance Class. When I went to insert the title and the url into the partition and row keys of the first-mentioned class, I found that there were conflicts because I did not use an MD5 algorithm to hash them. Originally, I had wanted to make a function but because I had been using this in many files I decided to create an MD5Hash object. 

-Section 2: StorageMaster Class
In the beginning, I had initialized a bunch of queues and tables in the web role. When I realized that I had to reinitialize all of them in the worker role too, I decided that it was too cumbersome not to make a class that would act as a "bag" where I could just pull the relevant storage item in question. This became the impetus for my StorageMaster Class. In this class, I had created a queue for unprocessed URLs, a queue to tell it when to stop and start, and an XML queue (this would provide visibility into whether the sitemaps were being parsed correctly, added after it became difficult to check the status). It also had tables for errors, processed urls, performances and statuses so that it would know what state it was in. All of these were private fields. The StorageMaster class also has public fields of the directives, CPU and RAM Counters and the initial seed robots.txt files because all of these were pertinent to the individual queues and tables storage. The class also has getters and setters and other functions that help complete the assignment.

-Section 3: WebCrawler Class
This class performs the bulk of the crawling work. First there is an intialize method where robots.txt is parsed for all initial sitemaps and the first-order urls from those sitemaps. A special condition is enforced: only sitemaps and urls from 2 months ago are loaded to the queue when the robots.txt is cnn.com whereas all the sites are loaded if the robots.txt belongs to bleacherreport.com. Disallowed urls are not added via local hashset. Once the queue has been seeded from the initial urls, the crawl method can take a url from the queue, get the page title, and, along with todays date, the url, and the index, add to the table. Then, all of its references are properly formatted, fed to a local hashset of visited urls to prevent duplicates from being inserted into the queue. All the Urls to the queue are indexed for easy retrieval. Any errors are logged.

-Section 4: WorkerRole
The primary function of this file is to store all of the performance information at regular intervals and, using the StorageManager's directives queue, direct the crawl to initialize and crawl when appropriate.

-Section 5: WebRole (axmx)
The primary function of this file is to communicate to the webrole when the cralwer should start, stop, clear all indexes, and when to report the status of the crawler which includes [State of each worker role web crawler (loading, crawling, idle), Machine counters (CPU Utilization%, RAM available), #URLs crawled, Last 10 URLs crawled, Size of queue (#urls left in pipeline to be crawled), Size of index (Table storage with our crawled data), Any errors and their URLs)].

-Section 6: Javascript and HTML
The primary function of this file is to translate user behavior from the front end dashboard such as clicking on start and clear to requests to the asmx file. Additionally, through ajax, we can get the aforementioned information and display it to the dashboard.